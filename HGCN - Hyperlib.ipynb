{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8859cdb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import tensorflow_datasets as tfds\n",
    "\n",
    "from hyperlib.manifold.lorentz import Lorentz\n",
    "from hyperlib.manifold.poincare import Poincare\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras.losses import CategoricalCrossentropy\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from spektral.data import SingleLoader\n",
    "from spektral.datasets.citation import Citation\n",
    "from spektral.transforms import LayerPreprocess\n",
    "from spektral.layers import GCNConv\n",
    "from spektral.models.gcn import GCN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6e7438dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import hyperlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8d654d79",
   "metadata": {},
   "outputs": [],
   "source": [
    "from hyperlib.nn.layers.linear import ActivationHyperbolic, LinearHyperbolic\n",
    "from hyperlib.nn.layers.graph import HGCLayer, HyperbolicAggregation, HGCNLP\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c1f1fcd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9ebe4a61",
   "metadata": {},
   "outputs": [],
   "source": [
    "#ConvHyperbolic(16, self.manifold, self.c0, self.c1, activation=\"relu\")\n",
    "hgc_layer = HGCLayer(100, Poincare(), 0.4, \"relu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b9421213",
   "metadata": {},
   "outputs": [],
   "source": [
    "from hyperlib.utils.data_utils import load_data, load_data_lp, mask_edges, process"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c22811d8",
   "metadata": {},
   "source": [
    "## Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "949e5d23",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Nalex\\Playgrounds\\HyperbolicPackage\\branch\\hyperlib\\hyperlib\\utils\\data_utils.py:198: FutureWarning: adjacency_matrix will return a scipy.sparse array instead of a matrix in Networkx 3.0.\n",
      "  adj = nx.adjacency_matrix(nx.from_dict_of_lists(graph))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(3664290, 3696)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = load_data_lp(\"cora\", True, \"resources\\\\data\\\\hgcn\\\\cora\")\n",
    "\n",
    "val_prop = 0.1\n",
    "test_prop = 0.2\n",
    "split_seed = 42\n",
    "normalize_adj = False\n",
    "normalize_feats = False\n",
    "\n",
    "adj = data['adj_train']\n",
    "\n",
    "adj_train, train_edges, train_edges_false, val_edges, val_edges_false, test_edges, test_edges_false = mask_edges(\n",
    "        adj, val_prop, test_prop, split_seed\n",
    ")\n",
    "data['adj_train'] = adj_train\n",
    "data['train_edges'], data['train_edges_false'] = train_edges, train_edges_false\n",
    "data['val_edges'], data['val_edges_false'] = val_edges, val_edges_false\n",
    "data['test_edges'], data['test_edges_false'] = test_edges, test_edges_false\n",
    "    \n",
    "data['adj_train_norm'], data['features'] = process(\n",
    "    data['adj_train'], data['features'], normalize_adj, normalize_feats\n",
    ")\n",
    "\n",
    "\n",
    "data.keys()\n",
    "\n",
    "data[\"adj_train\"]\n",
    "\n",
    "data['adj_train_norm']\n",
    "\n",
    "data[\"features\"]\n",
    "\n",
    "data['features']\n",
    "\n",
    "n_nodes, feat_dim = data['features'].shape\n",
    "\n",
    "n_nodes, feat_dim\n",
    "\n",
    "nb_false_edges = len(data['train_edges_false'])\n",
    "nb_edges = len(data['train_edges'])\n",
    "nb_false_edges, nb_edges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "4cec3f6a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2708, 1433])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['features'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "76fd2050",
   "metadata": {},
   "outputs": [],
   "source": [
    "hgcn = HGCNLP(1433)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "a4663163",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Exception encountered when calling layer \"hgc_layer_6\" (type HGCLayer).\n\nCan't convert non-rectangular Python sequence to Tensor.\n\nCall arguments received by layer \"hgc_layer_6\" (type HGCLayer):\n  • inputs=('tf.Tensor(shape=(2708, 1433), dtype=float32)', 'tensor(indices=tensor([[   0,    0,    0,  ..., 2706, 2707, 2707],\\n                       [ 633, 1862, 2582,  ..., 1473,  165,  598]]),\\n       values=tensor([1., 1., 1.,  ..., 1., 1., 1.]),\\n       size=(2708, 2708), nnz=7392, layout=torch.sparse_coo)')",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Input \u001b[1;32mIn [31]\u001b[0m, in \u001b[0;36m<cell line: 2>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(epochs):\n\u001b[0;32m      3\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEpoch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m----> 4\u001b[0m     \u001b[43mhgcn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mfeatures\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43madj_train_norm\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32md:\\pythonvirtualenvironments\\hyperbolic_playground\\lib\\site-packages\\keras\\utils\\traceback_utils.py:67\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     65\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:  \u001b[38;5;66;03m# pylint: disable=broad-except\u001b[39;00m\n\u001b[0;32m     66\u001b[0m   filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[1;32m---> 67\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28mNone\u001b[39m\n\u001b[0;32m     68\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m     69\u001b[0m   \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[1;32mD:\\Nalex\\Playgrounds\\HyperbolicPackage\\branch\\hyperlib\\hyperlib\\nn\\layers\\graph.py:67\u001b[0m, in \u001b[0;36mHGCNLP.call\u001b[1;34m(self, inputs)\u001b[0m\n\u001b[0;32m     65\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmanifold\u001b[38;5;241m.\u001b[39mexpmap0(x, c\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mc_map)\n\u001b[0;32m     66\u001b[0m \u001b[38;5;66;03m# Stack multiple hyperbolic graph convolution layers\u001b[39;00m\n\u001b[1;32m---> 67\u001b[0m x, adj \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconv0\u001b[49m\u001b[43m(\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43madj\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     68\u001b[0m x, adj \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconv1((x, adj))\n\u001b[0;32m     69\u001b[0m x, adj \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconv2((x, adj))\n",
      "File \u001b[1;32mD:\\Nalex\\Playgrounds\\HyperbolicPackage\\branch\\hyperlib\\hyperlib\\nn\\layers\\graph.py:36\u001b[0m, in \u001b[0;36mHGCLayer.call\u001b[1;34m(self, inputs)\u001b[0m\n\u001b[0;32m     33\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmanifold\u001b[38;5;241m.\u001b[39mlogmap0(x, c\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mc)\n\u001b[0;32m     35\u001b[0m \u001b[38;5;66;03m# Step 2 (attention-based neighborhood aggregation)\u001b[39;00m\n\u001b[1;32m---> 36\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinear_layer\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     37\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maggregation_layer((x, adj))\n\u001b[0;32m     39\u001b[0m \u001b[38;5;66;03m# Step 3 (non-linear activation with different curvatures)\u001b[39;00m\n",
      "File \u001b[1;32mD:\\Nalex\\Playgrounds\\HyperbolicPackage\\branch\\hyperlib\\hyperlib\\nn\\layers\\linear.py:21\u001b[0m, in \u001b[0;36mLinearHyperbolic.build\u001b[1;34m(self, batch_input_shape)\u001b[0m\n\u001b[0;32m     18\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mbuild\u001b[39m(\u001b[38;5;28mself\u001b[39m, batch_input_shape):\n\u001b[0;32m     19\u001b[0m     w_init \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mrandom_normal_initializer()\n\u001b[0;32m     20\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mkernel \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mVariable(\n\u001b[1;32m---> 21\u001b[0m         initial_value\u001b[38;5;241m=\u001b[39m\u001b[43mw_init\u001b[49m\u001b[43m(\u001b[49m\u001b[43mshape\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mbatch_input_shape\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43munits\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfloat64\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m, dtype\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfloat64\u001b[39m\u001b[38;5;124m\"\u001b[39m, trainable\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[0;32m     22\u001b[0m     )\n\u001b[0;32m     24\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39muse_bias:\n\u001b[0;32m     25\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbias \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39madd_weight(\n\u001b[0;32m     26\u001b[0m             name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbias\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m     27\u001b[0m             shape\u001b[38;5;241m=\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39munits),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     30\u001b[0m             trainable\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[0;32m     31\u001b[0m         )\n",
      "\u001b[1;31mValueError\u001b[0m: Exception encountered when calling layer \"hgc_layer_6\" (type HGCLayer).\n\nCan't convert non-rectangular Python sequence to Tensor.\n\nCall arguments received by layer \"hgc_layer_6\" (type HGCLayer):\n  • inputs=('tf.Tensor(shape=(2708, 1433), dtype=float32)', 'tensor(indices=tensor([[   0,    0,    0,  ..., 2706, 2707, 2707],\\n                       [ 633, 1862, 2582,  ..., 1473,  165,  598]]),\\n       values=tensor([1., 1., 1.,  ..., 1., 1., 1.]),\\n       size=(2708, 2708), nnz=7392, layout=torch.sparse_coo)')"
     ]
    }
   ],
   "source": [
    "epochs = 10\n",
    "for epoch in range(epochs):\n",
    "    print(f\"Epoch {epoch}\")\n",
    "    hgcn((data['features'], data['adj_train_norm']))\n",
    "    #grads = tape.gradient(loss_value, self.embedding.trainable_weights)\n",
    "    #optimizer.apply_gradients(zip(grads, self.embedding.trainable_weights))\n",
    "\n",
    "    #if step % 100 == 0:\n",
    "    #    log.info(\"Training loss (for one batch) at step %d: %.4f\"\n",
    "    #        % (step, float(loss_value)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
